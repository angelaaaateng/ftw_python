{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f5ad892d-d6a8-945a-015d-b0a16aff3bd4","id":"9GtqQweNc7FG"},"source":["<....Work in progress...>\n","\n","Thank you for opening this script!\n","\n","I have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts.\n","\n","Please **upvote** this kernel so that it reaches the top of the chart and is easily locatable by new users. Your comments on how we can improve this kernel is welcome. Thanks.\n","***\n","## Layout of the document\n","The prediction process is divided into two notebooks.\n","\n","Part 1 : Covers data statistics, data visualization, and feature selection : https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-on-feature-selection\n","\n","This notebook : Covers prediction using various algorithms \n","***\n","## Data statistics\n","* Shape\n","* Datatypes\n","* Description\n","* Skew\n","* Class distribution\n","\n","## Data Interaction\n","* Correlation\n","* Scatter plot\n","\n","## Data Visualization\n","* Box and density plots\n","* Grouping of one hot encoded attributes\n","\n","## Data Cleaning\n","* Remove unnecessary columns\n","\n","## Data Preparation\n","* Original\n","* Delete rows or impute values in case of missing\n","* StandardScaler\n","* MinMaxScaler\n","* Normalizer\n","\n","## Feature selection\n","* ExtraTreesClassifier\n","* GradientBoostingClassifier\n","* RandomForestClassifier\n","* XGBClassifier\n","* RFE\n","* SelectPercentile\n","* PCA\n","* PCA + SelectPercentile\n","* Feature Engineering\n","\n","## Evaluation, prediction, and analysis\n","* LDA (Linear algo)\n","* LR (Linear algo)\n","* KNN (Non-linear algo)\n","* CART (Non-linear algo)\n","* Naive Bayes (Non-linear algo)\n","* SVC (Non-linear algo)\n","* Bagged Decision Trees (Bagging)\n","* Random Forest (Bagging)\n","* Extra Trees (Bagging)\n","* AdaBoost (Boosting)\n","* Stochastic Gradient Boosting (Boosting)\n","* Voting Classifier (Voting)\n","* MLP (Deep Learning)\n","* XGBoost\n","\n","***"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b2c56171-517f-e38b-d72a-bf619947081b","id":"zCYjxTKqc7FM"},"source":["## Load raw data:\n","\n","Information about all the attributes can be found here:\n","\n","https://www.kaggle.com/c/forest-cover-type-prediction/data\n","\n","Learning: \n","We need to predict the 'Cover_Type' based on the other attributes. Hence, this is a classification problem where the target could belong to any of the seven classes."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"06ffea42-305e-1805-6380-b3d5ed0283f8","id":"Z9hlnz5Ac7FN","executionInfo":{"status":"ok","timestamp":1674843541144,"user_tz":-480,"elapsed":949,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["# Supress unnecessary warnings so that presentation looks clean\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Read raw data from the file\n","\n","import pandas #provides data structures to quickly analyze data\n","#Since this code runs on Kaggle server, train data can be accessed directly in the 'input' folder\n","dataset = pandas.read_csv(\"https://raw.githubusercontent.com/angelaaaateng/ftw_python/main/data/covertype_train.csv\") \n","\n","#Drop the first column 'Id' since it just has serial numbers. Not useful in the prediction process.\n","dataset = dataset.iloc[:,1:]"]},{"cell_type":"markdown","metadata":{"_cell_guid":"2c949a64-97fe-5d39-a4d2-f2eefb3ab73b","id":"tABp21s6c7FO"},"source":["## Data Cleaning\n","* Remove unnecessary columns"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"852062ac-990c-03d8-2af9-86a5318a3f63","colab":{"base_uri":"https://localhost:8080/"},"id":"gnCQIAlYc7FO","executionInfo":{"status":"ok","timestamp":1674843541145,"user_tz":-480,"elapsed":9,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}},"outputId":"3f893a99-0576-427c-aa70-48b8e97c0e95"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Soil_Type7', 'Soil_Type15']\n"]}],"source":["#Removal list initialize\n","rem = []\n","\n","#Add constant columns as they don't help in prediction process\n","for c in dataset.columns:\n","    if dataset[c].std() == 0: #standard deviation is zero\n","        rem.append(c)\n","\n","#drop the columns        \n","dataset.drop(rem,axis=1,inplace=True)\n","\n","print(rem)\n","\n","#Following columns are dropped"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b08bf340-b519-acbb-4907-2893e60dbe93","id":"AVt24fe_c7FP"},"source":["## Data Preparation\n","* Original\n","* Delete rows or impute values in case of missing\n","* StandardScaler\n","* MinMaxScaler\n","* Normalizer"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"8bd9157c-5c93-8488-9047-e8eeedb97ecd","id":"jmoGA8gcc7FQ","executionInfo":{"status":"ok","timestamp":1674843641041,"user_tz":-480,"elapsed":31,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#get the number of rows and columns\n","r, c = dataset.shape\n","\n","#get the list of columns\n","cols = dataset.columns\n","#create an array which has indexes of columns\n","i_cols = []\n","for i in range(0,c-1):\n","    i_cols.append(i)\n","#array of importance rank of all features  \n","ranks = []\n","\n","#Extract only the values\n","array = dataset.values\n","\n","#Y is the target column, X has the rest\n","X_orig = array[:,0:(c-1)]\n","Y = array[:,(c-1)]\n","\n","#Validation chunk size\n","val_size = 0.1\n","\n","#Use a common seed in all experiments so that same chunk is used for validation\n","seed = 0\n","\n","#Split the data into chunks\n","# from sklearn import cross_validation\n","from sklearn.model_selection import train_test_split\n","X_train, X_val, Y_train, Y_val = train_test_split(X_orig, Y, test_size=val_size, random_state=seed)\n","\n","#Import libraries for data transformations\n","# from sklearn.preprocessing import Imputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import Normalizer\n","\n","#All features\n","X_all = []\n","#Additionally we will make a list of subsets\n","X_all_add =[]\n","\n","#columns to be dropped\n","rem_cols = []\n","#indexes of columns to be dropped\n","i_rem = []\n","\n","#Add this version of X to the list \n","X_all.append(['Orig','All', X_train,X_val,1.0,cols[:c-1],rem_cols,ranks,i_cols,i_rem])\n","\n","#point where categorical data begins\n","size=10\n","\n","import numpy\n","\n","#Standardized\n","#Apply transform only for non-categorical data\n","X_temp = StandardScaler().fit_transform(X_train[:,0:size])\n","X_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n","#Concatenate non-categorical data and categorical\n","X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n","X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n","#Add this version of X to the list \n","X_all.append(['StdSca','All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n","\n","#MinMax\n","#Apply transform only for non-categorical data\n","X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n","X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n","#Concatenate non-categorical data and categorical\n","X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n","X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n","#Add this version of X to the list \n","X_all.append(['MinMax', 'All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n","\n","#Normalize\n","#Apply transform only for non-categorical data\n","X_temp = Normalizer().fit_transform(X_train[:,0:size])\n","X_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n","#Concatenate non-categorical data and categorical\n","X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n","X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n","#Add this version of X to the list \n","X_all.append(['Norm', 'All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n","\n","#Impute\n","#Imputer is not used as no data is missing\n","\n","#List of transformations\n","trans_list = []\n","\n","for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","    trans_list.append(trans)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b1b587c5-638e-fa8f-5558-deeb9ba903fa","id":"pOcmJw_Wc7FR"},"source":["## Feature Selection\n","Using the rankings produced in :\n","https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-on-feature-selection"]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"442389b8-bfbe-ec8b-746f-d5222b0f93f7","id":"SuefFYCRc7FS","executionInfo":{"status":"ok","timestamp":1674843646927,"user_tz":-480,"elapsed":679,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Select top 75%,50%,25%\n","ratio_list = [0.75,0.50,0.25]\n","\n","#Median of rankings for each column\n","unsorted_rank = [0,8,11,4,5,2,5,7.5,9.5,3,8,28.5,14.5,2,35,19.5,12,14,37,25.5,50,44,9,28,20.5,19.5,40,38,20,38,43,35,44,22,24,33,49,42,46,47,27.5,19,31.5,23,28,42,30.5,46,40,12,13,18]\n","\n","#List of feature selection models\n","feat = []\n","\n","#Add Median to the list \n","n = 'Median'\n","for val in ratio_list:\n","    feat.append([n,val])   \n","\n","for trans,s, X, X_val, d, cols, rem_cols, ra, i_cols, i_rem in X_all:\n","    #Create subsets of feature list based on ranking and ratio_list\n","    for name, v in feat:\n","        #Combine importance and index of the column in the array joined\n","        joined = []\n","        for i, pred in enumerate(unsorted_rank):\n","            joined.append([i,cols[i],pred])\n","        #Sort in descending order    \n","        joined_sorted = sorted(joined, key=lambda x: x[2])\n","        #Starting point of the columns to be dropped\n","        rem_start = int((v*(c-1)))\n","        #List of names of columns selected\n","        cols_list = []\n","        #Indexes of columns selected\n","        i_cols_list = []\n","        #Ranking of all the columns\n","        rank_list =[]\n","        #List of columns not selected\n","        rem_list = []\n","        #Indexes of columns not selected\n","        i_rem_list = []\n","        #Split the array. Store selected columns in cols_list and removed in rem_list\n","        for j, (i, col, x) in enumerate(list(joined_sorted)):\n","            #Store the rank\n","            rank_list.append([i,j])\n","            #Store selected columns in cols_list and indexes in i_cols_list\n","            if(j < rem_start):\n","                cols_list.append(col)\n","                i_cols_list.append(i)\n","            #Store not selected columns in rem_list and indexes in i_rem_list    \n","            else:\n","                rem_list.append(col)\n","                i_rem_list.append(i)    \n","        #Sort the rank_list and store only the ranks. Drop the index \n","        #Append model name, array, columns selected and columns to be removed to the additional list        \n","        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])"]},{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"ef7fb5a7-c7be-6615-6441-6a2974b2cf41","id":"qB22YOrOc7FT","executionInfo":{"status":"ok","timestamp":1674843646928,"user_tz":-480,"elapsed":76,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Import plotting library    \n","import matplotlib.pyplot as plt    \n","\n","#Dictionary to store the accuracies for all combinations \n","acc = {}\n","\n","#List of combinations\n","comb = []\n","\n","#Append name of transformation to trans_list\n","for trans in trans_list:\n","    acc[trans]=[]"]},{"cell_type":"markdown","metadata":{"_cell_guid":"bbe1f53e-df03-ae2e-9ac9-1e0e9a0fc448","id":"PJEoe4jrc7FT"},"source":["## Evaluation, prediction, and analysis\n","* LDA (Linear algo)"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"2c8b2d88-2666-f3a9-098e-dccca8babc33","id":"x1yiNeHjc7FU","executionInfo":{"status":"ok","timestamp":1674843649381,"user_tz":-480,"elapsed":2529,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of LinearDiscriminatAnalysis using all the views\n","\n","#Import the library\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","\n","#Set the base model\n","model = LinearDiscriminantAnalysis()\n","algo = \"LDA\"\n","\n","##Set figure size\n","#plt.rc(\"figure\", figsize=(25, 10))\n","\n","#Accuracy of the model using all features\n","for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","    model.fit(X[:,i_cols_list],Y_train)\n","    result = model.score(X_val[:,i_cols_list], Y_val)\n","    acc[trans].append(result)\n","    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","    #print(result)\n","comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n","        \n","#Accuracy of the model using a subset of features    \n","for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","    model.fit(X[:,i_cols_list],Y_train)\n","    result = model.score(X_val[:,i_cols_list], Y_val)\n","    acc[trans].append(result)\n","    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","    #print(result)\n","for v in ratio_list:\n","    comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","\n","#Best estimated performance is 65%. Occurs when all features are used and without any transformation!\n","#Performance of MinMax and Normalizer is very poor"]},{"cell_type":"markdown","metadata":{"_cell_guid":"39d77f0c-4d50-91ae-33ad-cf4efae8e87d","id":"lbUObr6wc7FU"},"source":["## Evaluation, prediction, and analysis\n","* LR (Linear algo)"]},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"03f35c5c-643f-f08b-e417-675a9319f5d2","id":"2lY5aD07c7FV","executionInfo":{"status":"ok","timestamp":1674843681395,"user_tz":-480,"elapsed":32033,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of LogisticRegression using all the views\n","\n","#Import the library\n","from sklearn.linear_model import LogisticRegression\n","\n","C_list = [100]\n","\n","for C in C_list:\n","    #Set the base model\n","    model = LogisticRegression(n_jobs=-1,random_state=seed,C=C)\n","   \n","    algo = \"LR\"\n","\n","    ##Set figure size\n","    #plt.rc(\"figure\", figsize=(25, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"All\",1.0))\n","\n","    #Accuracy of the model using a subset of features    \n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    for v in ratio_list:\n","        comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","      \n","#Best estimated performance is close to 67% with LR when C=100 and all attributes are considered and with standardized data\n","#Performance improves will increasing value of C\n","#Performance of Normalizer and MinMax Scaler is poor in general"]},{"cell_type":"markdown","metadata":{"_cell_guid":"53c0cc77-90eb-ea8e-e4e6-46fbbb59b411","id":"qhYOz-6Gc7FV"},"source":["## Evaluation, prediction, and analysis\n","* KNN (Non-linear algo)"]},{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"af974c32-efad-df50-3921-47f5dca39214","id":"b-udkqqWc7FV","executionInfo":{"status":"ok","timestamp":1674843689033,"user_tz":-480,"elapsed":7686,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of KNN Classifier using all the views\n","\n","#Import the library\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","n_list = [1]\n","\n","for n_neighbors in n_list:\n","    #Set the base model\n","    model = KNeighborsClassifier(n_jobs=-1,n_neighbors=n_neighbors)\n","   \n","    algo = \"KNN\"\n","\n","    ##Set figure size\n","    #plt.rc(\"figure\", figsize=(25, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_neighbors,\"All\",1.0))\n","\n","    #Accuracy of the model using a subset of features    \n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    for v in ratio_list:\n","        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_neighbors,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n"," \n","#Best estimated performance is close to 86% when n_neighbors=1 and normalizer is used"]},{"cell_type":"markdown","metadata":{"_cell_guid":"5aa2fc20-122f-db7a-3f9a-98a8aac9fb87","id":"7s1CH0pKc7FW"},"source":["## Evaluation, prediction, and analysis\n","* Naive Bayes"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"21cb14f7-1d2e-cd10-63b8-472ebc4f5ac9","id":"YSKU1PYTc7FW","executionInfo":{"status":"ok","timestamp":1674843689034,"user_tz":-480,"elapsed":33,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of Naive Bayes using all the views\n","\n","#Import the library\n","from sklearn.naive_bayes import GaussianNB\n","\n","#Set the base model\n","model = GaussianNB()\n","algo = \"NB\"\n","\n","##Set figure size\n","#plt.rc(\"figure\", figsize=(25, 10))\n","\n","#Accuracy of the model using all features\n","for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","    model.fit(X[:,i_cols_list],Y_train)\n","    result = model.score(X_val[:,i_cols_list], Y_val)\n","    acc[trans].append(result)\n","    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","    #print(result)\n","comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n","        \n","#Accuracy of the model using a subset of features    \n","for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","    model.fit(X[:,i_cols_list],Y_train)\n","    result = model.score(X_val[:,i_cols_list], Y_val)\n","    acc[trans].append(result)\n","    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","    #print(result)\n","for v in ratio_list:\n","    comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","\n","#Best estimated performance is close to 64%. Original with 50% subset outperfoms all transformations of NB"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e7d94b6d-6b7e-f5c2-23a0-1961b49cef2d","id":"x9a0ozIAc7FW"},"source":["## Evaluation, prediction, and analysis\n","* CART (Non-linear algo)"]},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"7716c4f2-0075-6535-e244-5711c38c5dff","id":"lOoXIWS9c7FX","executionInfo":{"status":"ok","timestamp":1674843691881,"user_tz":-480,"elapsed":2416,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of CART using all the views\n","\n","#Import the library\n","from sklearn.tree import DecisionTreeClassifier\n","\n","d_list = [13]\n","\n","for max_depth in d_list:\n","    #Set the base model\n","    model = DecisionTreeClassifier(random_state=seed,max_depth=max_depth)\n","   \n","    algo = \"CART\"\n","\n","    #Set figure size\n","    plt.rc(\"figure\", figsize=(15, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"All\",1.0))\n","\n","    #Accuracy of the model using a subset of features    \n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    for v in ratio_list:\n","        comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","    \n","#Best estimated performance is close to 79% when max_depth=13 and for Original"]},{"cell_type":"markdown","metadata":{"_cell_guid":"db295e4c-47b9-3341-2c96-41395d2be871","id":"CKKCUKooc7FX"},"source":["## Evaluation, prediction, and analysis\n","* SVM (Non-linear algo)"]},{"cell_type":"code","execution_count":16,"metadata":{"_cell_guid":"c2e1b555-209f-5917-8760-5507a29b98a8","id":"lI4VMMYdc7FX","executionInfo":{"status":"ok","timestamp":1674843717144,"user_tz":-480,"elapsed":25286,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of SVM using all the views\n","\n","#Import the library\n","from sklearn.svm import SVC\n","\n","c_list = [10]\n","\n","for C in c_list:\n","    #Set the base model\n","    model = SVC(random_state=seed,C=C)\n","\n","    algo = \"SVM\"\n","\n","    #Set figure size\n","    #plt.rc(\"figure\", figsize=(15, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"All\",1.0))\n","\n","    ##Accuracy of the model using a subset of features    \n","    #for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","    #    model.fit(X[:,i_cols_list],Y_train)\n","    #    result = model.score(X_val[:,i_cols_list], Y_val)\n","    #    acc[trans].append(result)\n","    #    print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","    #    print(result)\n","    #for v in ratio_list:\n","    #    comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","\n","#Training time is very high compared to other algos\n","#Performance is very poor for original. Shows the importance of data transformation\n","#Best estimated performance is close to 77% when C=10 and for StandardScaler with 0.25 subset"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8007309e-ffd5-6aac-4e2b-46d9783a0c6e","id":"PPGOIacvc7FX"},"source":["## Evaluation, prediction, and analysis\n","* Bagged Decision Trees (Bagging)"]},{"cell_type":"code","execution_count":17,"metadata":{"_cell_guid":"9e128756-f838-f8b0-9671-bfffcc42ce96","id":"aq7W_GBHc7FY","executionInfo":{"status":"ok","timestamp":1674843842703,"user_tz":-480,"elapsed":125596,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of Bagged Decision Trees using all the views\n","\n","#Import the library\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","#Base estimator\n","base_estimator = DecisionTreeClassifier(random_state=seed,max_depth=13)\n","\n","n_list = [100]\n","\n","for n_estimators in n_list:\n","    #Set the base model\n","    model = BaggingClassifier(n_jobs=-1,base_estimator=base_estimator, n_estimators=n_estimators, random_state=seed)\n","   \n","    algo = \"Bag\"\n","\n","    #Set figure size\n","    plt.rc(\"figure\", figsize=(20, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n","\n","    #Accuracy of the model using a subset of features    \n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    for v in ratio_list:\n","        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","\n","#Best estimated performance is close to 82% when n_estimators is 100 for Original"]},{"cell_type":"markdown","metadata":{"_cell_guid":"f2d0e657-57d5-d773-77df-be31fba0e74a","id":"pA7B9N9Mc7FY"},"source":["## Evaluation, prediction, and analysis\n","* Random Forest (Bagging)"]},{"cell_type":"code","execution_count":18,"metadata":{"_cell_guid":"d4c28025-9b76-f634-f33b-1045a4a3dae9","id":"R8mvvIQac7FY","executionInfo":{"status":"ok","timestamp":1674843877971,"user_tz":-480,"elapsed":35350,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of Random Forest using all the views\n","\n","#Import the library\n","from sklearn.ensemble import RandomForestClassifier\n","\n","n_list = [100]\n","\n","for n_estimators in n_list:\n","    #Set the base model\n","    model = RandomForestClassifier(n_jobs=-1,n_estimators=n_estimators, random_state=seed)\n","   \n","    algo = \"RF\"\n","\n","    #Set figure size\n","    plt.rc(\"figure\", figsize=(20, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n","\n","    #Accuracy of the model using a subset of features    \n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    for v in ratio_list:\n","        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","\n","#Best estimated performance is close to 85% when n_estimators is 100"]},{"cell_type":"markdown","metadata":{"_cell_guid":"01de07bd-ff0d-1d7f-8be0-278088394a05","id":"B6z_jF2yc7FY"},"source":["## Evaluation, prediction, and analysis\n","* Extra Trees (Bagging)"]},{"cell_type":"code","execution_count":19,"metadata":{"_cell_guid":"08928963-6c7a-1df4-40aa-ee650995ae6d","id":"3C-bnqTpc7FZ","executionInfo":{"status":"ok","timestamp":1674843898384,"user_tz":-480,"elapsed":20460,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of Extra Trees using all the views\n","\n","#Import the library\n","from sklearn.ensemble import ExtraTreesClassifier\n","\n","n_list = [100]\n","\n","for n_estimators in n_list:\n","    #Set the base model\n","    model = ExtraTreesClassifier(n_jobs=-1,n_estimators=n_estimators, random_state=seed)\n","   \n","    algo = \"ET\"\n","\n","    #Set figure size\n","    plt.rc(\"figure\", figsize=(20, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n","\n","    #Accuracy of the model using a subset of features    \n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    for v in ratio_list:\n","        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","\n","#Best estimated performance is close to 88% when n_estimators is 100 , StdScaler with 0.75"]},{"cell_type":"markdown","metadata":{"_cell_guid":"417e7f27-53fb-658c-466b-e05be2f29b98","id":"FwPz_tvqc7FZ"},"source":["## Evaluation, prediction, and analysis\n","* AdaBoost (Boosting)"]},{"cell_type":"code","execution_count":20,"metadata":{"_cell_guid":"86125f08-0f3b-bc79-4953-2a961a66beef","id":"BM1-aITGc7FZ","executionInfo":{"status":"ok","timestamp":1674843935971,"user_tz":-480,"elapsed":37613,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of AdaBoost ensemble using all the views\n","\n","#Import the library\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","n_list = [100]\n","\n","for n_estimators in n_list:\n","    #Set the base model\n","    model = AdaBoostClassifier(n_estimators=n_estimators, random_state=seed)\n","   \n","    algo = \"Ada\"\n","\n","    #Set figure size\n","    plt.rc(\"figure\", figsize=(20, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n","\n","    #Accuracy of the model using a subset of features    \n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    for v in ratio_list:\n","        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","\n","#Best estimated performance is close to 38% when n_estimators is 100"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a35afac7-15fb-dc01-3588-8cc3f31bb781","id":"fLZBOGsXc7FZ"},"source":["## Evaluation, prediction, and analysis\n","* Gradient Boosting (Boosting)"]},{"cell_type":"code","execution_count":21,"metadata":{"_cell_guid":"8468b0bb-59e9-b5f9-9b64-ed6c8dc7b21d","id":"iRku7SqPc7FZ","executionInfo":{"status":"ok","timestamp":1674844297359,"user_tz":-480,"elapsed":361408,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of Stochastic Gradient Boosting using all the views\n","\n","#Import the library\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","d_list = [9]\n","\n","for max_depth in d_list:\n","    #Set the base model\n","    model = GradientBoostingClassifier(max_depth=max_depth, random_state=seed)\n","   \n","    algo = \"SGB\"\n","\n","    #Set figure size\n","    plt.rc(\"figure\", figsize=(20, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"All\",1.0))\n","\n","    ##Accuracy of the model using a subset of features    \n","    #for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","    #    model.fit(X[:,i_cols_list],Y_train)\n","    #    result = model.score(X_val[:,i_cols_list], Y_val)\n","    #    acc[trans].append(result)\n","    #    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","    #    #print(result)\n","    #for v in ratio_list:\n","    #    comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","\n","#training time is too high\n","#Best estimated performance is close to 86% when depth is 7"]},{"cell_type":"markdown","metadata":{"_cell_guid":"4bba0620-cedf-41e4-b9cb-99ab46a198c7","id":"NcUXNLXIc7Fa"},"source":["## Evaluation, prediction, and analysis\n","* Voting Classifier (Voting)"]},{"cell_type":"code","execution_count":22,"metadata":{"_cell_guid":"8ce798b5-dc06-bb2b-47d1-4973e60f31e7","id":"w8yQr6dVc7Fa","executionInfo":{"status":"ok","timestamp":1674844501802,"user_tz":-480,"elapsed":204477,"user":{"displayName":"Amber Teng","userId":"07980583791026710498"}}},"outputs":[],"source":["#Evaluation of various combinations of Voting Classifier using all the views\n","\n","#Import the library\n","from sklearn.ensemble import VotingClassifier\n","\n","list_estimators =[]\n","\n","estimators = []\n","model1 = ExtraTreesClassifier(n_jobs=-1,n_estimators=100, random_state=seed)\n","estimators.append(('et', model1))\n","model2 = RandomForestClassifier(n_jobs=-1,n_estimators=100, random_state=seed)\n","estimators.append(('rf', model2))\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","base_estimator = DecisionTreeClassifier(random_state=seed,max_depth=13)\n","model3 = BaggingClassifier(n_jobs=-1,base_estimator=base_estimator, n_estimators=100, random_state=seed)\n","estimators.append(('bag', model3))\n","\n","list_estimators.append(['Voting',estimators])\n","\n","for name, estimators in list_estimators:\n","    #Set the base model\n","    model = VotingClassifier(estimators=estimators, n_jobs=-1)\n","   \n","    algo = name\n","\n","    #Set figure size\n","    plt.rc(\"figure\", figsize=(20, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n","\n","    #Accuracy of the model using a subset of features    \n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    for v in ratio_list:\n","        comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","\n","#Best estimated performance is close to 86%"]},{"cell_type":"markdown","metadata":{"_cell_guid":"974030fd-cc78-a2b1-24e0-6bbf87e3fa70","id":"QRILXEVpc7Fa"},"source":["## Evaluation, prediction, and analysis\n","* XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe04235b-c66c-22a1-9bae-a4124d2183be","id":"iVBGn7ngc7Fa"},"outputs":[],"source":["#Evaluation of various combinations of XG Boost using all the views\n","\n","#Import the library\n","from xgboost import XGBClassifier\n","\n","n_list = [300]\n","\n","for n_estimators in n_list:\n","    #Set the base model\n","    model = XGBClassifier(n_estimators=n_estimators, seed=seed,subsample=0.25)\n","   \n","    algo = \"XGB\"\n","\n","    #Set figure size\n","    plt.rc(\"figure\", figsize=(20, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n","\n","    #Accuracy of the model using a subset of features    \n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","        model.fit(X[:,i_cols_list],Y_train)\n","        result = model.score(X_val[:,i_cols_list], Y_val)\n","        acc[trans].append(result)\n","        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","        #print(result)\n","    for v in ratio_list:\n","        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n","    \n","##Plot the accuracies of all combinations\n","#fig, ax = plt.subplots()\n","##Plot each transformation\n","#for trans in trans_list:\n","#        plt.plot(acc[trans])\n","##Set the tick names to names of combinations\n","#ax.set_xticks(range(len(comb)))\n","#ax.set_xticklabels(comb,rotation='vertical')\n","##Display the plot\n","#plt.legend(trans_list,loc='best')    \n","##Plot the accuracy for all combinations\n","#plt.show()    \n","\n","#Best estimated performance is close to 80% when n_estimators is 300, sub_sample=0.25 , subset=0.75"]},{"cell_type":"markdown","metadata":{"_cell_guid":"70b659c5-87ae-2e3a-62f5-2668239ce4b8","id":"4Q7K7hr-c7Fa"},"source":["## Evaluation, prediction, and analysis\n","* Multi-layer perceptrons (Deep learning)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a48bf097-92a5-4f2f-585c-7c9d8267faf8","id":"-eT6OdTXc7Fb"},"outputs":[],"source":["#Evaluation of baseline model of MLP using all the views\n","\n","#Import libraries for deep learning\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","#Import libraries for encoding\n","from keras.utils import np_utils\n","from sklearn.preprocessing import LabelEncoder\n","\n","#no. of output classes\n","y = 7\n","\n","#random state\n","numpy.random.seed(seed)\n","\n","# one hot encode class values\n","encoder = LabelEncoder()\n","Y_train_en = encoder.fit_transform(Y_train)\n","Y_train_hot = np_utils.to_categorical(Y_train_en,y) \n","Y_val_en = encoder.fit_transform(Y_val)\n","Y_val_hot = np_utils.to_categorical(Y_val_en,y) \n","\n","\n","# define baseline model\n","def baseline(v):\n","     # create model\n","     model = Sequential()\n","     model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n","     model.add(Dense(y, init='normal', activation='sigmoid'))\n","     # Compile model\n","     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","     return model\n","\n","# define smaller model\n","def smaller(v):\n"," # create model\n"," model = Sequential()\n"," model.add(Dense(v*(c-1)/2, input_dim=v*(c-1), init='normal', activation='relu'))\n"," model.add(Dense(y, init='normal', activation='sigmoid'))\n"," # Compile model\n"," model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"," return model\n","\n","# define deeper model\n","def deeper(v):\n"," # create model\n"," model = Sequential()\n"," model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n"," model.add(Dense(v*(c-1)/2, init='normal', activation='relu'))\n"," model.add(Dense(y, init='normal', activation='sigmoid'))\n"," # Compile model\n"," model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"," return model\n","\n","# Optimize using dropout and decay\n","from keras.optimizers import SGD\n","from keras.layers import Dropout\n","from keras.constraints import maxnorm\n","\n","def dropout(v):\n","    #create model\n","    model = Sequential()\n","    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu',W_constraint=maxnorm(3)))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(v*(c-1)/2, init='normal', activation='relu', W_constraint=maxnorm(3)))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(y, init='normal', activation='sigmoid'))\n","    # Compile model\n","    sgd = SGD(lr=0.1,momentum=0.9,decay=0.0,nesterov=False)\n","    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","    return model\n","\n","# define decay model\n","def decay(v):\n","    # create model\n","    model = Sequential()\n","    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n","    model.add(Dense(y, init='normal', activation='sigmoid'))\n","    # Compile model\n","    sgd = SGD(lr=0.1,momentum=0.8,decay=0.01,nesterov=False)\n","    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","    return model\n","    \n","est_list = [('MLP',baseline),('smaller',smaller),('deeper',deeper),('dropout',dropout),('decay',decay)]\n","\n","for name, est in est_list:\n"," \n","    algo = name\n","\n","    #Set figure size\n","    plt.rc(\"figure\", figsize=(20, 10))\n","\n","    #Accuracy of the model using all features\n","    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n","        model = KerasClassifier(build_fn=est, v=v, nb_epoch=10, verbose=0)\n","        model.fit(X[:,i_cols_list],Y_train_hot)\n","        result = model.score(X_val[:,i_cols_list], Y_val_hot)\n","        acc[trans].append(result)\n","    #    print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","    #    print(result)\n","    comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n","\n","    ##Accuracy of the model using a subset of features    \n","    #for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","    #    model = KerasClassifier(build_fn=est, v=v, nb_epoch=10, verbose=0)\n","    #    model.fit(X[:,i_cols_list],Y_train_hot)\n","    #    result = model.score(X_val[:,i_cols_list], Y_val_hot)\n","    #    acc[trans].append(result)\n","    #    print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n","    #    print(result)\n","    #for v in ratio_list:\n","    #    comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n","\n","#Plot the accuracies of all combinations\n","fig, ax = plt.subplots()\n","#Plot each transformation\n","for trans in trans_list:\n","        plt.plot(acc[trans])\n","#Set the tick names to names of combinations\n","ax.set_xticks(range(len(comb)))\n","ax.set_xticklabels(comb,rotation='vertical')\n","#Display the plot\n","plt.legend(trans_list,loc='best')    \n","#Plot the accuracy for all combinations\n","plt.show()    \n","\n","# Best estimated performance is 71% \n","# Performance is poor is general. Data transformations make a huge difference."]},{"cell_type":"markdown","metadata":{"_cell_guid":"81915fe8-ff04-8b87-7c53-33f7ebf85127","id":"Lj5N6sKoc7Fb"},"source":["##Make Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c0a79ee-461f-1f28-c47f-7453892cd3ad","id":"5mMvoABtc7Fb"},"outputs":[],"source":["# Make predictions using Extra Tress Classifier + 0.5 subset as it gave the best estimated performance\n","\n","n_estimators = 100\n","\n","#Obtain the list of indexes for the required model\n","indexes = []\n","for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n","    if v == 0.5:\n","        if trans == 'Orig':\n","            indexes = i_cols_list\n","            break\n","\n","#Best model definition\n","best_model = ExtraTreesClassifier(n_jobs=-1,n_estimators=n_estimators)\n","best_model.fit(X_orig[:,indexes],Y)\n","\n","#Read test dataset\n","dataset_test = pandas.read_csv(\"../input/test.csv\")\n","#Drop unnecessary columns\n","ID = dataset_test['Id']\n","dataset_test.drop('Id',axis=1,inplace=True)\n","dataset_test.drop(rem,axis=1,inplace=True)\n","X_test = dataset_test.values\n","\n","#Make predictions using the best model\n","predictions = best_model.predict(X_test[:,indexes])\n","# Write submissions to output file in the correct format\n","with open(\"submission.csv\", \"w\") as subfile:\n","    subfile.write(\"Id,Cover_Type\\n\")\n","    for i, pred in enumerate(list(predictions)):\n","        subfile.write(\"%s,%s\\n\"%(ID[i],pred))"]},{"cell_type":"code","source":[],"metadata":{"id":"wem6Lq88ek9w"},"execution_count":null,"outputs":[]}],"metadata":{"_change_revision":688,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}