{"cells":[{"cell_type":"markdown","metadata":{"id":"xHOC81CJdBoB"},"source":["# Multiple Linear Regression in Statsmodels\n","\n","## Introduction\n","\n","In this lecture, you'll learn how to run your first multiple linear regression model.\n","\n","## Objectives\n","You will be able to:\n","* Use statsmodels to fit a multiple linear regression model\n","* Evaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n","\n","## Statsmodels for multiple linear regression\n","\n","This lesson will be more of a code-along, where you'll walk through a multiple linear regression model using both statsmodels and scikit-learn. \n","\n","The regression model determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In intro algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\n","\n","## Auto-mpg data\n","* Creating dummy variables for each categorical feature\n","* Log-transforming select continuous predictors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgLTfcm9dBoF"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","data = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/dsc-multiple-linear-regression-in-statsmodels-london-ds-02172020/master/auto-mpg.csv') \n","data['horsepower'].astype(str).astype(int)\n","\n","acc = data['acceleration']\n","logdisp = np.log(data['displacement'])\n","loghorse = np.log(data['horsepower'])\n","logweight= np.log(data['weight'])\n","\n","scaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\n","scaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\n","scaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\n","scaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n","\n","data_fin = pd.DataFrame([])\n","data_fin['acc'] = scaled_acc\n","data_fin['disp'] = scaled_disp\n","data_fin['horse'] = scaled_horse\n","data_fin['weight'] = scaled_weight\n","cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n","yr_dummies = pd.get_dummies(data['model year'], prefix='yr', drop_first=True)\n","orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)\n","mpg = data['mpg']\n","data_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fHNdwaidBoH","outputId":"b0894eb9-fb5c-4f75-a8fb-70dbadc4afe0"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 392 entries, 0 to 391\n","Data columns (total 23 columns):\n"," #   Column  Non-Null Count  Dtype  \n","---  ------  --------------  -----  \n"," 0   mpg     392 non-null    float64\n"," 1   acc     392 non-null    float64\n"," 2   disp    392 non-null    float64\n"," 3   horse   392 non-null    float64\n"," 4   weight  392 non-null    float64\n"," 5   cyl_4   392 non-null    uint8  \n"," 6   cyl_5   392 non-null    uint8  \n"," 7   cyl_6   392 non-null    uint8  \n"," 8   cyl_8   392 non-null    uint8  \n"," 9   yr_71   392 non-null    uint8  \n"," 10  yr_72   392 non-null    uint8  \n"," 11  yr_73   392 non-null    uint8  \n"," 12  yr_74   392 non-null    uint8  \n"," 13  yr_75   392 non-null    uint8  \n"," 14  yr_76   392 non-null    uint8  \n"," 15  yr_77   392 non-null    uint8  \n"," 16  yr_78   392 non-null    uint8  \n"," 17  yr_79   392 non-null    uint8  \n"," 18  yr_80   392 non-null    uint8  \n"," 19  yr_81   392 non-null    uint8  \n"," 20  yr_82   392 non-null    uint8  \n"," 21  orig_2  392 non-null    uint8  \n"," 22  orig_3  392 non-null    uint8  \n","dtypes: float64(5), uint8(18)\n","memory usage: 22.3 KB\n"]}],"source":["data_fin.info()"]},{"cell_type":"markdown","metadata":{"id":"djwWCil0dBoH"},"source":["For now, let's simplify the model and only inlude `'acc'`, `'horse'` and the three `'orig'` categories in our final data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7h0QpLPvdBoI","outputId":"56f4d65f-a6aa-4b50-d288-99d917a403e4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mpg</th>\n","      <th>acceleration</th>\n","      <th>weight</th>\n","      <th>orig_2</th>\n","      <th>orig_3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>18.0</td>\n","      <td>0.238095</td>\n","      <td>0.720986</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>15.0</td>\n","      <td>0.208333</td>\n","      <td>0.908047</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>18.0</td>\n","      <td>0.178571</td>\n","      <td>0.651205</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16.0</td>\n","      <td>0.238095</td>\n","      <td>0.648095</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17.0</td>\n","      <td>0.148810</td>\n","      <td>0.664652</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    mpg  acceleration    weight  orig_2  orig_3\n","0  18.0      0.238095  0.720986       0       0\n","1  15.0      0.208333  0.908047       0       0\n","2  18.0      0.178571  0.651205       0       0\n","3  16.0      0.238095  0.648095       0       0\n","4  17.0      0.148810  0.664652       0       0"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\n","data_ols.head()"]},{"cell_type":"markdown","metadata":{"id":"vKEPvvjkdBoI"},"source":["## A linear model using statsmodels"]},{"cell_type":"markdown","metadata":{"id":"nbd1R8RudBoJ"},"source":["Now, let's use the `statsmodels.api` to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFknISWtdBoJ"},"outputs":[],"source":["import statsmodels.api as sm\n","from statsmodels.formula.api import ols"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yc_T3Y-FdBoJ"},"outputs":[],"source":["formula = 'mpg ~ acceleration+weight+orig_2+orig_3'\n","model = ols(formula=formula, data=data_ols).fit()"]},{"cell_type":"markdown","metadata":{"id":"x2KM7uV2dBoK"},"source":["Having to type out all the predictors isn't practical when you have many. Another better way than to type them all out is to seperate out the outcome variable `'mpg'` out of your DataFrame, and use the a `'+'.join()` command on the predictors, as done below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egOHKTMVdBoK"},"outputs":[],"source":["outcome = 'mpg'\n","predictors = data_ols.drop('mpg', axis=1)\n","pred_sum = '+'.join(predictors.columns)\n","formula = outcome + '~' + pred_sum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jak4ly6AdBoK","outputId":"35a15d92-a4e6-459a-b656-67d8dbaf65e7"},"outputs":[{"data":{"text/html":["<table class=\"simpletable\">\n","<caption>OLS Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.726</td> \n","</tr>\n","<tr>\n","  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.723</td> \n","</tr>\n","<tr>\n","  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   256.7</td> \n","</tr>\n","<tr>\n","  <th>Date:</th>             <td>Fri, 13 Jan 2023</td> <th>  Prob (F-statistic):</th> <td>1.86e-107</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                 <td>14:51:01</td>     <th>  Log-Likelihood:    </th> <td> -1107.2</td> \n","</tr>\n","<tr>\n","  <th>No. Observations:</th>      <td>   392</td>      <th>  AIC:               </th> <td>   2224.</td> \n","</tr>\n","<tr>\n","  <th>Df Residuals:</th>          <td>   387</td>      <th>  BIC:               </th> <td>   2244.</td> \n","</tr>\n","<tr>\n","  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>Intercept</th>    <td>   20.7608</td> <td>    0.688</td> <td>   30.181</td> <td> 0.000</td> <td>   19.408</td> <td>   22.113</td>\n","</tr>\n","<tr>\n","  <th>acceleration</th> <td>    5.0494</td> <td>    1.389</td> <td>    3.634</td> <td> 0.000</td> <td>    2.318</td> <td>    7.781</td>\n","</tr>\n","<tr>\n","  <th>weight</th>       <td>   -5.8764</td> <td>    0.282</td> <td>  -20.831</td> <td> 0.000</td> <td>   -6.431</td> <td>   -5.322</td>\n","</tr>\n","<tr>\n","  <th>orig_2</th>       <td>    0.4124</td> <td>    0.639</td> <td>    0.645</td> <td> 0.519</td> <td>   -0.844</td> <td>    1.669</td>\n","</tr>\n","<tr>\n","  <th>orig_3</th>       <td>    1.7218</td> <td>    0.653</td> <td>    2.638</td> <td> 0.009</td> <td>    0.438</td> <td>    3.005</td>\n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","  <th>Omnibus:</th>       <td>37.427</td> <th>  Durbin-Watson:     </th> <td>   0.840</td>\n","</tr>\n","<tr>\n","  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  55.989</td>\n","</tr>\n","<tr>\n","  <th>Skew:</th>          <td> 0.648</td> <th>  Prob(JB):          </th> <td>6.95e-13</td>\n","</tr>\n","<tr>\n","  <th>Kurtosis:</th>      <td> 4.322</td> <th>  Cond. No.          </th> <td>    8.47</td>\n","</tr>\n","</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."],"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                    mpg   R-squared:                       0.726\n","Model:                            OLS   Adj. R-squared:                  0.723\n","Method:                 Least Squares   F-statistic:                     256.7\n","Date:                Fri, 13 Jan 2023   Prob (F-statistic):          1.86e-107\n","Time:                        14:51:01   Log-Likelihood:                -1107.2\n","No. Observations:                 392   AIC:                             2224.\n","Df Residuals:                     387   BIC:                             2244.\n","Df Model:                           4                                         \n","Covariance Type:            nonrobust                                         \n","================================================================================\n","                   coef    std err          t      P>|t|      [0.025      0.975]\n","--------------------------------------------------------------------------------\n","Intercept       20.7608      0.688     30.181      0.000      19.408      22.113\n","acceleration     5.0494      1.389      3.634      0.000       2.318       7.781\n","weight          -5.8764      0.282    -20.831      0.000      -6.431      -5.322\n","orig_2           0.4124      0.639      0.645      0.519      -0.844       1.669\n","orig_3           1.7218      0.653      2.638      0.009       0.438       3.005\n","==============================================================================\n","Omnibus:                       37.427   Durbin-Watson:                   0.840\n","Prob(Omnibus):                  0.000   Jarque-Bera (JB):               55.989\n","Skew:                           0.648   Prob(JB):                     6.95e-13\n","Kurtosis:                       4.322   Cond. No.                         8.47\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","\"\"\""]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["model = ols(formula=formula, data=data_ols).fit()\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"VhO4ddNbdBoL"},"source":["Or even easier, simply use the `ols()` function from `statsmodels.api`. The advantage is that you don't have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your `predictors` DataFrame so it includes a constant term. You can do this using `.add_constant`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kKMMf_WgdBoL","outputId":"ce1611ca-e8be-4305-df4c-23224af84c26"},"outputs":[{"data":{"text/html":["<table class=\"simpletable\">\n","<caption>OLS Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.726</td> \n","</tr>\n","<tr>\n","  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.723</td> \n","</tr>\n","<tr>\n","  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   256.7</td> \n","</tr>\n","<tr>\n","  <th>Date:</th>             <td>Fri, 13 Jan 2023</td> <th>  Prob (F-statistic):</th> <td>1.86e-107</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                 <td>14:51:03</td>     <th>  Log-Likelihood:    </th> <td> -1107.2</td> \n","</tr>\n","<tr>\n","  <th>No. Observations:</th>      <td>   392</td>      <th>  AIC:               </th> <td>   2224.</td> \n","</tr>\n","<tr>\n","  <th>Df Residuals:</th>          <td>   387</td>      <th>  BIC:               </th> <td>   2244.</td> \n","</tr>\n","<tr>\n","  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>const</th>        <td>   20.7608</td> <td>    0.688</td> <td>   30.181</td> <td> 0.000</td> <td>   19.408</td> <td>   22.113</td>\n","</tr>\n","<tr>\n","  <th>acceleration</th> <td>    5.0494</td> <td>    1.389</td> <td>    3.634</td> <td> 0.000</td> <td>    2.318</td> <td>    7.781</td>\n","</tr>\n","<tr>\n","  <th>weight</th>       <td>   -5.8764</td> <td>    0.282</td> <td>  -20.831</td> <td> 0.000</td> <td>   -6.431</td> <td>   -5.322</td>\n","</tr>\n","<tr>\n","  <th>orig_2</th>       <td>    0.4124</td> <td>    0.639</td> <td>    0.645</td> <td> 0.519</td> <td>   -0.844</td> <td>    1.669</td>\n","</tr>\n","<tr>\n","  <th>orig_3</th>       <td>    1.7218</td> <td>    0.653</td> <td>    2.638</td> <td> 0.009</td> <td>    0.438</td> <td>    3.005</td>\n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","  <th>Omnibus:</th>       <td>37.427</td> <th>  Durbin-Watson:     </th> <td>   0.840</td>\n","</tr>\n","<tr>\n","  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  55.989</td>\n","</tr>\n","<tr>\n","  <th>Skew:</th>          <td> 0.648</td> <th>  Prob(JB):          </th> <td>6.95e-13</td>\n","</tr>\n","<tr>\n","  <th>Kurtosis:</th>      <td> 4.322</td> <th>  Cond. No.          </th> <td>    8.47</td>\n","</tr>\n","</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."],"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                    mpg   R-squared:                       0.726\n","Model:                            OLS   Adj. R-squared:                  0.723\n","Method:                 Least Squares   F-statistic:                     256.7\n","Date:                Fri, 13 Jan 2023   Prob (F-statistic):          1.86e-107\n","Time:                        14:51:03   Log-Likelihood:                -1107.2\n","No. Observations:                 392   AIC:                             2224.\n","Df Residuals:                     387   BIC:                             2244.\n","Df Model:                           4                                         \n","Covariance Type:            nonrobust                                         \n","================================================================================\n","                   coef    std err          t      P>|t|      [0.025      0.975]\n","--------------------------------------------------------------------------------\n","const           20.7608      0.688     30.181      0.000      19.408      22.113\n","acceleration     5.0494      1.389      3.634      0.000       2.318       7.781\n","weight          -5.8764      0.282    -20.831      0.000      -6.431      -5.322\n","orig_2           0.4124      0.639      0.645      0.519      -0.844       1.669\n","orig_3           1.7218      0.653      2.638      0.009       0.438       3.005\n","==============================================================================\n","Omnibus:                       37.427   Durbin-Watson:                   0.840\n","Prob(Omnibus):                  0.000   Jarque-Bera (JB):               55.989\n","Skew:                           0.648   Prob(JB):                     6.95e-13\n","Kurtosis:                       4.322   Cond. No.                         8.47\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","\"\"\""]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import statsmodels.api as sm\n","predictors_int = sm.add_constant(predictors)\n","model = sm.OLS(data['mpg'],predictors_int).fit()\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Gai0Y5KZdBoL"},"source":["## Interpretation\n","\n","Just like for single multiple regression, the coefficients for the model should be interpreted as \"how does $y$ change for each additional unit $X$\"? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is \"how does $y$ change for each additional unit $X'$\", where $X'$ is the (log- and min-max, standardized,...) transformed data matrix.\n","\n","## Linear regression using scikit-learn\n","\n","You can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn't have some statistical metrics like the p-values of the parameter estimates readily available. For a more *ad-hoc* comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GmmP0lI_dBoL"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEzcXFghdBoM","outputId":"5f7e9b76-b58f-42b2-ac40-5b9caa89a7d2"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"],"text/plain":["LinearRegression()"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["y = data_ols['mpg']\n","linreg = LinearRegression()\n","linreg.fit(predictors, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHYGW8hmdBoM","outputId":"2745e9c2-76d4-4b55-ee22-fcb9ebb42644"},"outputs":[{"data":{"text/plain":["array([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# coefficients\n","linreg.coef_"]},{"cell_type":"markdown","metadata":{"id":"r33YBKL2dBoM"},"source":["The intercept of the model is stored in the `.intercept_` attribute."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TICpL8V_dBoM","outputId":"90f14a07-9bbb-4005-a60b-6b4977e671ac"},"outputs":[{"data":{"text/plain":["20.76075708082184"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# intercept\n","linreg.intercept_"]},{"cell_type":"markdown","metadata":{"id":"vTME6a86dBoN"},"source":["## Summary\n","\n","Congrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWz92UVzdBoN"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[{"file_id":"https://github.com/angelaaaateng/ftw_python/blob/main/B7_EDA/Multiple_Linear_Reg.ipynb","timestamp":1673618930857}]}},"nbformat":4,"nbformat_minor":0}